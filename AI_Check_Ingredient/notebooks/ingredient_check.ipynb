{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae678121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Supplement Synonym Search (Canonical-Only)\n",
      "‚úÖ Loaded 1090 canonical entries from cleaned_supplements_highacc.json.\n",
      "‚öôÔ∏è Loading model: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Building embeddings from canonical names...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:13<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Embedding complete in 13.4s for 1090 entries\n",
      "‚úÖ FAISS index built and cached.\n",
      "\n",
      "‚úÖ Ready. Type any ingredient name (or 'exit' to quit).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for '':\n",
      "======================================================================\n",
      "Canonical: gaba\n",
      "  Scores ‚Üí semantic: 0.915, lexical: 0.0, final: 0.641\n",
      "----------------------------------------------------------------------\n",
      "Canonical: tein\n",
      "  Scores ‚Üí semantic: 0.896, lexical: 0.0, final: 0.627\n",
      "----------------------------------------------------------------------\n",
      "Canonical: oleamid\n",
      "  Scores ‚Üí semantic: 0.886, lexical: 0.0, final: 0.62\n",
      "----------------------------------------------------------------------\n",
      "Canonical: tiggarn t\n",
      "  Scores ‚Üí semantic: 0.855, lexical: 0.0, final: 0.599\n",
      "----------------------------------------------------------------------\n",
      "Canonical: vitpil\n",
      "  Scores ‚Üí semantic: 0.847, lexical: 0.0, final: 0.593\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Results for 'alba':\n",
      "======================================================================\n",
      "Canonical: abies alba\n",
      "  Scores ‚Üí semantic: 0.912, lexical: 0.571, final: 0.81\n",
      "----------------------------------------------------------------------\n",
      "Canonical: salix alba\n",
      "  Scores ‚Üí semantic: 0.908, lexical: 0.571, final: 0.807\n",
      "----------------------------------------------------------------------\n",
      "Canonical: basella alba\n",
      "  Scores ‚Üí semantic: 0.859, lexical: 0.5, final: 0.751\n",
      "----------------------------------------------------------------------\n",
      "Canonical: morus alba\n",
      "  Scores ‚Üí semantic: 0.826, lexical: 0.571, final: 0.75\n",
      "----------------------------------------------------------------------\n",
      "Canonical: gaba\n",
      "  Scores ‚Üí semantic: 0.748, lexical: 0.75, final: 0.749\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Results for 'Acer':\n",
      "======================================================================\n",
      "Canonical: acer nigrum\n",
      "  Scores ‚Üí semantic: 0.676, lexical: 0.4, final: 0.593\n",
      "----------------------------------------------------------------------\n",
      "Canonical: acer saccharum\n",
      "  Scores ‚Üí semantic: 0.689, lexical: 0.333, final: 0.582\n",
      "----------------------------------------------------------------------\n",
      "Canonical: tein\n",
      "  Scores ‚Üí semantic: 0.711, lexical: 0.25, final: 0.572\n",
      "----------------------------------------------------------------------\n",
      "Canonical: v nderot\n",
      "  Scores ‚Üí semantic: 0.665, lexical: 0.333, final: 0.566\n",
      "----------------------------------------------------------------------\n",
      "Canonical: tagetes erecta\n",
      "  Scores ‚Üí semantic: 0.712, lexical: 0.222, final: 0.565\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Results for 'Vit A':\n",
      "======================================================================\n",
      "Canonical: tiggarn t\n",
      "  Scores ‚Üí semantic: 0.893, lexical: 0.286, final: 0.711\n",
      "----------------------------------------------------------------------\n",
      "Canonical: vitl k\n",
      "  Scores ‚Üí semantic: 0.772, lexical: 0.545, final: 0.704\n",
      "----------------------------------------------------------------------\n",
      "Canonical: pramiracetam\n",
      "  Scores ‚Üí semantic: 0.902, lexical: 0.235, final: 0.702\n",
      "----------------------------------------------------------------------\n",
      "Canonical: vitpil\n",
      "  Scores ‚Üí semantic: 0.846, lexical: 0.364, final: 0.702\n",
      "----------------------------------------------------------------------\n",
      "Canonical: oleamid\n",
      "  Scores ‚Üí semantic: 0.924, lexical: 0.167, final: 0.697\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Results for 'Vit A':\n",
      "======================================================================\n",
      "Canonical: tiggarn t\n",
      "  Scores ‚Üí semantic: 0.893, lexical: 0.286, final: 0.711\n",
      "----------------------------------------------------------------------\n",
      "Canonical: vitl k\n",
      "  Scores ‚Üí semantic: 0.772, lexical: 0.545, final: 0.704\n",
      "----------------------------------------------------------------------\n",
      "Canonical: pramiracetam\n",
      "  Scores ‚Üí semantic: 0.902, lexical: 0.235, final: 0.702\n",
      "----------------------------------------------------------------------\n",
      "Canonical: vitpil\n",
      "  Scores ‚Üí semantic: 0.846, lexical: 0.364, final: 0.702\n",
      "----------------------------------------------------------------------\n",
      "Canonical: oleamid\n",
      "  Scores ‚Üí semantic: 0.924, lexical: 0.167, final: 0.697\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Results for 'Vitamen B':\n",
      "======================================================================\n",
      "Canonical: vitamin b1\n",
      "  Scores ‚Üí semantic: 0.857, lexical: 0.632, final: 0.789\n",
      "----------------------------------------------------------------------\n",
      "Canonical: vitamin b2\n",
      "  Scores ‚Üí semantic: 0.842, lexical: 0.632, final: 0.779\n",
      "----------------------------------------------------------------------\n",
      "Canonical: vitamin a\n",
      "  Scores ‚Üí semantic: 0.828, lexical: 0.667, final: 0.779\n",
      "----------------------------------------------------------------------\n",
      "Canonical: vitamin e\n",
      "  Scores ‚Üí semantic: 0.819, lexical: 0.667, final: 0.773\n",
      "----------------------------------------------------------------------\n",
      "Canonical: vitamin b7\n",
      "  Scores ‚Üí semantic: 0.821, lexical: 0.632, final: 0.764\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Supplement Synonym Search - Canonical-Only Training\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Optional FAISS acceleration\n",
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FAISS_AVAILABLE = False\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "DATA_PATH      = \"cleaned_supplements_highacc.json\"   # canonical_name field required\n",
    "MODEL_NAME     = \"paraphrase-multilingual-MiniLM-L12-v2\"  # base model\n",
    "# MODEL_NAME   = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"  # higher accuracy, slower\n",
    "\n",
    "EMB_PATH       = \"embeddings_canonical.npy\"\n",
    "INDEX_PATH     = \"index_canonical.faiss\"\n",
    "LOOKUP_PATH    = \"index_lookup_canonical.csv\"\n",
    "META_PATH      = \"index_meta_canonical.json\"\n",
    "\n",
    "TOP_K_DEFAULT  = 5\n",
    "RECALL_K       = 50\n",
    "ALPHA_SEM      = 0.70\n",
    "MIN_CONFIDENCE = 0.55\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Helpers\n",
    "# ===============================================================\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normalization for both query and canonical names.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", s.lower())\n",
    "    s = re.sub(r\"[\\u2212\\u2010-\\u2015]\", \"-\", s)\n",
    "    s = re.sub(r\"[^a-z0-9 \\-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def load_canonical_data():\n",
    "    \"\"\"Load dataset and keep only canonical_name column.\"\"\"\n",
    "    if DATA_PATH.lower().endswith(\".json\"):\n",
    "        with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        df = pd.DataFrame(data)\n",
    "    elif DATA_PATH.lower().endswith(\".csv\"):\n",
    "        df = pd.read_csv(DATA_PATH, encoding=\"utf-8\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {DATA_PATH}\")\n",
    "\n",
    "    if \"canonical_name\" not in df.columns:\n",
    "        raise ValueError(\"Input file must contain 'canonical_name'.\")\n",
    "\n",
    "    # Keep only canonical_name column\n",
    "    df = df[[\"canonical_name\"]].copy()\n",
    "    df[\"canonical_name\"] = df[\"canonical_name\"].astype(str).apply(normalize_text)\n",
    "    df = df[df[\"canonical_name\"].notna() & (df[\"canonical_name\"] != \"\")]\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(df)} canonical entries from {DATA_PATH}.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_or_load_index(df):\n",
    "    \"\"\"Build embeddings + FAISS index using canonical_name only.\"\"\"\n",
    "    print(\"‚öôÔ∏è Loading model:\", MODEL_NAME)\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    # Check cache\n",
    "    if os.path.exists(META_PATH) and os.path.exists(EMB_PATH) and os.path.exists(LOOKUP_PATH):\n",
    "        try:\n",
    "            with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "                meta = json.load(f)\n",
    "            if meta.get(\"row_count\") == len(df) and meta.get(\"model\") == MODEL_NAME:\n",
    "                print(\"üîÅ Loading cached embeddings/index...\")\n",
    "                embeddings = np.load(EMB_PATH)\n",
    "                df_lookup = pd.read_csv(LOOKUP_PATH)\n",
    "                index = None\n",
    "                if FAISS_AVAILABLE and os.path.exists(INDEX_PATH):\n",
    "                    index = faiss.read_index(INDEX_PATH)\n",
    "                    print(\"‚úÖ FAISS index loaded from disk.\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è FAISS not installed or missing; using cosine similarity.\")\n",
    "                return model, embeddings, index, df_lookup\n",
    "        except Exception:\n",
    "            print(\"‚ôªÔ∏è Cache mismatch or read error ‚Äî rebuilding index.\")\n",
    "\n",
    "    # Build from scratch\n",
    "    print(\"‚öôÔ∏è Building embeddings from canonical names...\")\n",
    "    texts = df[\"canonical_name\"].tolist()\n",
    "\n",
    "    t0 = time.time()\n",
    "    embeddings = model.encode(\n",
    "        texts, show_progress_bar=True, normalize_embeddings=True\n",
    "    )\n",
    "    print(f\"‚è±Ô∏è Embedding complete in {time.time() - t0:.1f}s for {len(texts)} entries\")\n",
    "\n",
    "    np.save(EMB_PATH, embeddings)\n",
    "    df.to_csv(LOOKUP_PATH, index=False)\n",
    "    with open(META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"model\": MODEL_NAME, \"row_count\": len(df)}, f)\n",
    "\n",
    "    if FAISS_AVAILABLE:\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        index.add(np.array(embeddings, dtype=\"float32\"))\n",
    "        faiss.write_index(index, INDEX_PATH)\n",
    "        print(\"‚úÖ FAISS index built and cached.\")\n",
    "    else:\n",
    "        index = None\n",
    "        print(\"‚ö†Ô∏è FAISS not installed ‚Äî using cosine similarity at query time.\")\n",
    "\n",
    "    return model, embeddings, index, df\n",
    "\n",
    "\n",
    "def search_name(query, model, embeddings, index, df_lookup, top_k=TOP_K_DEFAULT):\n",
    "    \"\"\"Search using canonical-only embeddings.\"\"\"\n",
    "    q_norm = normalize_text(query)\n",
    "    q_emb = model.encode([q_norm], normalize_embeddings=True)\n",
    "\n",
    "    # Step 1: Semantic retrieval\n",
    "    if FAISS_AVAILABLE and index is not None:\n",
    "        scores, idx = index.search(np.array(q_emb, dtype=\"float32\"), min(RECALL_K, len(df_lookup)))\n",
    "        idx, scores = idx[0], scores[0]\n",
    "    else:\n",
    "        sims = cosine_similarity(q_emb, embeddings)[0]\n",
    "        idx = np.argsort(sims)[::-1][:min(RECALL_K, len(df_lookup))]\n",
    "        scores = sims[idx]\n",
    "\n",
    "    # Step 2: Combine with lexical (fuzzy) re-rank\n",
    "    results = []\n",
    "    for i, s in zip(idx, scores):\n",
    "        name = df_lookup.iloc[i][\"canonical_name\"]\n",
    "        lex = fuzz.token_sort_ratio(query, name) / 100\n",
    "        final = ALPHA_SEM * float(s) + (1 - ALPHA_SEM) * lex\n",
    "        if final >= MIN_CONFIDENCE:\n",
    "            results.append({\n",
    "                \"canonical_name\": name,\n",
    "                \"semantic\": round(float(s), 3),\n",
    "                \"lexical\": round(lex, 3),\n",
    "                \"score\": round(final, 3)\n",
    "            })\n",
    "\n",
    "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# CLI\n",
    "# ===============================================================\n",
    "def main():\n",
    "    print(\"üß† Supplement Synonym Search (Canonical-Only)\")\n",
    "    df = load_canonical_data()\n",
    "    model, embeddings, index, df_lookup = build_or_load_index(df)\n",
    "\n",
    "    print(\"\\n‚úÖ Ready. Type any ingredient name (or 'exit' to quit).\")\n",
    "    while True:\n",
    "        query = input(\"\\nüîç Enter ingredient name: \").strip()\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"üëã Exiting program. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        results = search_name(query, model, embeddings, index, df_lookup)\n",
    "\n",
    "        print(f\"\\nResults for '{query}':\")\n",
    "        print(\"=\" * 70)\n",
    "        if not results:\n",
    "            print(\"No confident match (below threshold). Try another term.\")\n",
    "        else:\n",
    "            for r in results:\n",
    "                print(\n",
    "                    f\"Canonical: {r['canonical_name']}\\n\"\n",
    "                    f\"  Scores ‚Üí semantic: {r['semantic']}, lexical: {r['lexical']}, final: {r['score']}\\n\"\n",
    "                    f\"{'-'*70}\"\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
